- name: Make sure cluster_master is up
  ping:
  delegate_to: "{{ cluster_master }}"
  run_once: true

- name: Check for required variables
  fail: msg="Variable {{ item }} must be defined to use this playbook"
  #when: (item not in vars) or (item is undefined)
  when: item is undefined
  with_items:
  - cluster_master
  - volume
  - gluster_volumes
  - gluster_volumes[volume].device
  - gluster_volumes[volume].group
  - gluster_volumes[volume].size

- set_fact:
    device: "{{ gluster_volumes[volume].device }}"
    size: "{{ gluster_volumes[volume].size }}"

- name: Create volume group
  lvg:
    pvs: "{{ device }}"
    vg: "{{ volume }}"

- name: Create thin pool
  command: "lvcreate --thin {{ volume }}/pool
                     -l 100%FREE
                     --chunksize 256k
                     --poolmetadatasize 16G
                     --zero n"
  args:
    creates: "/dev/mapper/{{ volume }}-pool"

- name: Create brick LV
  command: "lvcreate --thin
                     --name {{ volume }}
                     --virtualsize {{ size }}
                     {{ volume }}/pool"
  args:
    creates: "/dev/mapper/{{ volume }}-{{ volume }}"

- name: Create brick file system
  filesystem:
    dev: "/dev/mapper/{{ volume }}-{{ volume }}"
    fstype: xfs
    opts: -i size=512

- name: Create brick mount directory
  file:
    path: "/bricks/{{ volume }}"
    state: directory

- name: Mount brick
  mount:
    fstype: xfs
    opts: inode64
    path: /bricks/{{ volume }}
    src: /dev/mapper/{{ volume }}-{{ volume }}
    state: mounted

- name: Create brick root directory
  file:
    path: "/bricks/{{ volume }}/brick"
    state: directory

- run_once: true
  delegate_to: "{{ cluster_master }}"
  block:
  - name: Create volume
    gluster_volume:
      state: present
      name: "{{ volume }}"
      bricks: "/bricks/{{ volume }}/brick"
      replicas: 3
      start_on_create: no
      options:
        client.ssl: "on"
        server.ssl: "on"
      cluster: "{{ groups[gluster_volumes[volume].group] |
                   map('extract', hostvars, 'ansible_fqdn') |
                   list }}"

  - name: Start volume
    gluster_volume:
      state: started
      name: "{{ volume }}"

  # We set hard limit to 100 so that the soft limit percentage == actual
  # number.
  - name: Set snapshot hard limit
    shell: "echo y | gluster snapshot config {{ volume }}
            snap-max-hard-limit 100"

  - name: Set snapshot soft limit
    shell: "echo y | gluster snapshot config snap-max-soft-limit
            {{ max_gluster_snaphots }}"

  - name: Check status of auto-delete
    shell: "gluster snapshot config | grep auto-delete | awk '{ print $3 }'"
    changed_when: false
    register: snap_auto_delete_status

  - name: Enable snapshot auto-delete
    command: "gluster snapshot config auto-delete enable"
    when: snap_auto_delete_status.stdout != "enable"

  - name: Check if gluster_shared_storage is enabled
    shell: "gluster volume get all cluster.enable-shared-storage |
            grep cluster.enable-shared-storage | awk '{ print $2 }'"
    changed_when: false
    register: shared_storage_status

  - name: Enable gluster_shared_storage
    command: "gluster volume set all cluster.enable-shared-storage enable"
    when: shared_storage_status.stdout != "enable"
    register: shared_storage_just_enabled

- name: Wait until gluster_shared_storage is mounted
  command: "grep -q /run/gluster/shared_storage /proc/mounts"
  register: result
  delay: 3
  retries: 100
  until: result.rc == 0
  when: shared_storage_just_enabled.changed

# Manually remove fstab entry because mount module will add its own
- name: Remove shared_storage fstab entry
  lineinfile:
    path: /etc/fstab
    regexp: '/run/gluster/shared_storage/'
    state: absent

- name: Make sure gluster_shared_storage is mounted
  mount:
    fstype: glusterfs
    path: /var/run/gluster/shared_storage/
    src: "{{ ansible_fqdn }}:/gluster_shared_storage"
    state: mounted

- name: Make sure gluster_shared_storage service is enabled
  systemd:
    name: glusterfssharedstorage
    enabled: true
    daemon-reload: true

- name: Adjust SELinux to allow cron to access gluster_shared_storage
  seboolean:
    name: cron_system_cronjob_use_shares
    persistent: yes
    state: yes

# The scheduler needes to be init'd from each node, but it also does
# cluster-wide operations that fail if done simultaneously. We use a random
# delay to stagger things.
- name: Initialize snapshot scheduler
  command: "snap_scheduler.py init"
  args:
    creates: /etc/cron.d/gcron_update_task
  register: result
  delay: "{{ 10 | random }}"
  retries: 100
  until: result.rc == 0

- run_once: true
  delegate_to: "{{ cluster_master }}"
  block:
  - name: Check if snap scheduling is enabled
    stat:
      path: /var/run/gluster/shared_storage/snaps/glusterfs_snap_cron_tasks
    register: snap_lnk

  - name: Enable snap scheduling
    command: "snap_scheduler.py enable"
    when: snap_lnk.stat.islnk is defined and snap_lnk.stat.islnk == false

  - name: Check for daily snapshot job
    command: "snap_scheduler.py list"
    changed_when: false
    register: result

  # Take a daily snapshot at some random minute past midnight
  - name: Create daily snapshot job
    command: "snap_scheduler.py add 'daily_{{ volume }}' '{{ 59 | random }} 0 * * *' '{{ volume }}'"
    when: result.stdout.find('daily_' ~ volume) == -1
