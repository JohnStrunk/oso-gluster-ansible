- name: Make sure cluster_master is up
  ping:
  delegate_to: "{{ cluster_master }}"
  run_once: true

- name: Check for required variables
  fail: msg="Variable {{ item }} must be defined to use this playbook"
  #when: (item not in vars) or (item is undefined)
  when: item is undefined
  with_items:
  - cluster_master
  - volume
  - gluster_volumes
  - gluster_volumes[volume].device
  - gluster_volumes[volume].group
  - gluster_volumes[volume].size

- set_fact:
    device: "{{ gluster_volumes[volume].device }}"
    size: "{{ gluster_volumes[volume].size }}"

- name: Create volume group
  lvg:
    pvs: "{{ device }}"
    vg: "{{ volume }}"

- name: Create thin pool
  command: "lvcreate --thin {{ volume }}/pool
                     -l 100%FREE
                     --chunksize 256k
                     --poolmetadatasize 16G
                     --zero n"
  args:
    creates: "/dev/mapper/{{ volume }}-pool"

- name: Create brick LV
  command: "lvcreate --thin
                     --name {{ volume }}
                     --virtualsize {{ size }}
                     {{ volume }}/pool"
  args:
    creates: "/dev/mapper/{{ volume }}-{{ volume }}"

- name: Create brick file system
  filesystem:
    dev: "/dev/mapper/{{ volume }}-{{ volume }}"
    fstype: xfs
    opts: -i size=512

- name: Create brick mount directory
  file:
    path: "/bricks/{{ volume }}"
    state: directory

- name: Mount brick
  mount:
    fstype: xfs
    opts: inode64,discard,prjquota,x-systemd.device-timeout=10min
    path: /bricks/{{ volume }}
    src: /dev/mapper/{{ volume }}-{{ volume }}
    state: mounted

- name: Create brick root directory
  file:
    path: "/bricks/{{ volume }}/brick"
    state: directory

- run_once: true
  delegate_to: "{{ cluster_master }}"
  block:
  - name: Create volume
    gluster_volume:
      state: present
      name: "{{ volume }}"
      bricks: "/bricks/{{ volume }}/brick"
      replicas: 3
      start_on_create: no
      options:
        client.ssl: "on"
        server.ssl: "on"
        performance.nl-cache: "on"
        performance.md-cache-timeout: "300"
        performance.cache-refresh-timeout: "60"
        performance.cache-size: "134217728"
      cluster: "{{ groups[gluster_volumes[volume].group] |
                   map('extract', hostvars, 'ansible_fqdn') |
                   list }}"

  - name: Start volume
    gluster_volume:
      state: started
      name: "{{ volume }}"

  # We enable profiling so that the gluster pmda can grab the stats
  - name: Enable volume profiling
    command: "gluster volume profile {{ volume }} start"
    register: result
    failed_when: "'started' not in result.stderr and
                  'successful' not in result.stdout"
    changed_when: "'successful' in result.stdout"

  # We set hard limit to 100 so that the soft limit percentage == actual
  # number.
  - name: Set snapshot hard limit
    shell: "echo y | gluster snapshot config {{ volume }}
            snap-max-hard-limit 100"

  - name: Set snapshot soft limit
    shell: "echo y | gluster snapshot config snap-max-soft-limit
            {{ max_gluster_snaphots }}"

  - name: Check status of auto-delete
    shell: "gluster snapshot config | grep auto-delete | awk '{ print $3 }'"
    changed_when: false
    register: snap_auto_delete_status

  - name: Enable snapshot auto-delete
    command: "gluster snapshot config auto-delete enable"
    when: snap_auto_delete_status.stdout != "enable"

  - name: Check if gluster_shared_storage is enabled
    shell: "gluster volume get all cluster.enable-shared-storage |
            grep cluster.enable-shared-storage | awk '{ print $2 }'"
    changed_when: false
    register: shared_storage_status

  - name: Enable gluster_shared_storage
    command: "gluster volume set all cluster.enable-shared-storage enable"
    when: shared_storage_status.stdout != "enable"
    register: shared_storage_just_enabled

- name: Wait until gluster_shared_storage is mounted
  command: "grep -q /run/gluster/shared_storage /proc/mounts"
  register: result
  delay: 3
  retries: 100
  until: result.rc == 0
  when: shared_storage_just_enabled.changed

# Manually remove fstab entry because mount module will add its own
- name: Remove shared_storage fstab entry
  lineinfile:
    path: /etc/fstab
    regexp: '/run/gluster/shared_storage/'
    state: absent

- name: Make sure gluster_shared_storage is mounted
  mount:
    fstype: glusterfs
    path: /var/run/gluster/shared_storage/
    src: "{{ ansible_fqdn }}:/gluster_shared_storage"
    state: mounted

- name: Make sure gluster_shared_storage service is enabled
  systemd:
    name: glusterfssharedstorage
    enabled: true
    daemon-reload: true

- name: Adjust SELinux to allow cron to access gluster_shared_storage
  seboolean:
    name: cron_system_cronjob_use_shares
    persistent: yes
    state: yes

# We serialize this task using
# https://github.com/ansible/ansible/issues/12170#issuecomment-372263149
# so that we can avoid a race in scheduler init
- name: Initialize snapshot scheduler
  include_tasks: "init_scheduler.yml"
  with_items: "{{ play_hosts }}"
  when: "hostvars[item].inventory_hostname == inventory_hostname"

- run_once: true
  delegate_to: "{{ cluster_master }}"
  block:
  - name: Check if snap scheduling is enabled
    shell: "snap_scheduler.py status | awk '{ print $5 }'"
    register: snap_status

  - name: Enable snap scheduling
    command: "snap_scheduler.py enable"
    when: "snap_status.stdout != 'Enabled'"

  - name: Check for daily snapshot job
    command: "snap_scheduler.py list"
    changed_when: false
    register: result

  # Take a daily snapshot at some random minute past midnight
  - name: Create daily snapshot job
    command: "snap_scheduler.py add 'daily_{{ volume }}' '{{ 59 | random }} 0 * * *' '{{ volume }}'"
    when: result.stdout.find('daily_' ~ volume) == -1
